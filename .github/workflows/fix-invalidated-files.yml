name: Fix Invalidated Files

on:
  workflow_dispatch:

permissions:
  contents: write

concurrency:
  group: fix-invalidated-files
  cancel-in-progress: false

jobs:
  process-invalidated-files:
    name: Process and Fix Invalidated Files
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.x'
      
      - name: Install dependencies
        run: pip install pyyaml jsonschema
      
      - name: Process and fix invalidated files
        id: process
        run: |
          python3 << 'PYTHON_SCRIPT'
          import os
          import json
          import yaml
          import shutil
          import re
          from pathlib import Path
          from datetime import datetime, timezone

          # Configuration
          INVALIDATED_DIR = Path("invalidated")
          PROPOSALS_DIR = Path("proposals")
          REPORTS_DIR = Path("reports")
          ACTIONS_LOG_DIR = REPORTS_DIR / "actions"
          SCHEMAS_DIR = Path("schemas")
          ARTIFACTS_DIR = Path("artifacts")
          PROCESSED_DIR = Path("processed")

          # Schema configuration
          SCHEMA_CONFIG = {
              "evidence_item": {
                  "id_field": "evidence_id",
                  "required": ["evidence_id", "section", "claim_summary", "evidence_type", "source", "confidence_level", "editor_status"],
                  "evidence_types": ["statute", "budget", "grant", "administrative_rule", "field_validation"],
                  "confidence_levels": ["high", "medium", "low"],
                  "editor_statuses": ["pending", "accepted", "corrected", "nullified"]
              },
              "money_flow": {
                  "id_field": "flow_id",
                  "required": ["flow_id", "source", "intermediary", "destination", "amount", "fund_type", "fiscal_year", "restrictions", "statutory_basis", "editor_status"],
                  "fund_types": ["state", "federal", "settlement"],
                  "editor_statuses": ["pending", "accepted", "corrected", "nullified"]
              },
              "field_validation": {
                  "id_field": "fv_id",
                  "required": ["fv_id", "jurisdiction", "validating_entity", "alignment_status", "evidence_basis", "disclosure_level", "editor_status"],
                  "alignment_statuses": ["open", "mixed", "captured"],
                  "disclosure_levels": ["restricted"],
                  "editor_statuses": ["pending", "accepted", "corrected", "nullified"]
              },
              "authority_reference": {
                  "id_field": "authority_id",
                  "required": ["authority_id", "authority_type", "citation", "administering_body", "governs", "effects", "editor_status"],
                  "authority_types": ["statute", "regulation", "policy"],
                  "editor_statuses": ["pending", "accepted", "corrected", "nullified"]
              }
          }

          class RunLog:
              """Manages detailed run logging."""
              def __init__(self):
                  self.timestamp = datetime.now(timezone.utc)
                  self.timestamp_str = self.timestamp.strftime("%Y-%m-%dT%H-%M-%S")
                  self.before_state = {}
                  self.processed_files = []
                  self.failed_sources = []  # Track sources that failed processing
                  self.quarantined_files = []  # Track files moved to quarantine
                  self.deleted_files = []
                  self.deleted_dirs = []
                  self.created_files = []
                  self.errors = []
                  self.reconciliation = {}
              
              def capture_before_state(self):
                  """Capture complete inventory of invalidated/ before processing."""
                  self.before_state = {
                      "captured_at": self.timestamp.isoformat(),
                      "directories": {},
                      "total_files": 0,
                      "total_dirs": 0
                  }
                  
                  if not INVALIDATED_DIR.exists():
                      return
                  
                  for schema_type in ["evidence_item", "money_flow", "field_validation", "authority_reference"]:
                      subdir = INVALIDATED_DIR / schema_type
                      if subdir.exists():
                          dir_info = {
                              "files": [],
                              "subdirectories": {}
                          }
                          for item in subdir.iterdir():
                              if item.is_file():
                                  if item.name != ".gitkeep":
                                      dir_info["files"].append({
                                          "name": item.name,
                                          "size": item.stat().st_size,
                                          "modified": datetime.fromtimestamp(item.stat().st_mtime, tz=timezone.utc).isoformat()
                                      })
                                      self.before_state["total_files"] += 1
                              elif item.is_dir():
                                  subdir_files = []
                                  for f in item.rglob("*"):
                                      if f.is_file():
                                          subdir_files.append({
                                              "path": str(f.relative_to(item)),
                                              "size": f.stat().st_size,
                                              "modified": datetime.fromtimestamp(f.stat().st_mtime, tz=timezone.utc).isoformat()
                                          })
                                          self.before_state["total_files"] += 1
                                  dir_info["subdirectories"][item.name] = {
                                      "files": subdir_files,
                                      "file_count": len(subdir_files)
                                  }
                                  self.before_state["total_dirs"] += 1
                          self.before_state["directories"][schema_type] = dir_info
              
              def save(self):
                  """Save run log to reports/actions/."""
                  ACTIONS_LOG_DIR.mkdir(parents=True, exist_ok=True)
                  
                  log_data = {
                      "run_timestamp": self.timestamp.isoformat(),
                      "before_state": self.before_state,
                      "summary": {
                          "files_processed": len(self.processed_files),
                          "files_created": len(self.created_files),
                          "files_deleted": len(self.deleted_files),
                          "directories_deleted": len(self.deleted_dirs),
                          "files_quarantined": len(self.quarantined_files),
                          "files_failed": len(self.failed_sources),
                          "errors": len(self.errors)
                      },
                      "processed_files": self.processed_files,
                      "failed_sources": self.failed_sources,
                      "quarantined_files": self.quarantined_files,
                      "created_files": self.created_files,
                      "deleted_files": self.deleted_files,
                      "deleted_directories": self.deleted_dirs,
                      "errors": self.errors,
                      "reconciliation": self.reconciliation
                  }
                  
                  log_file = ACTIONS_LOG_DIR / f"run_log_{self.timestamp_str}.json"
                  with open(log_file, 'w', encoding='utf-8') as f:
                      json.dump(log_data, f, indent=2, ensure_ascii=False)
                      f.write('\n')
                  
                  print(f"\nüìã Run log saved to: {log_file}")
                  return log_file

          def load_yaml_multi_doc(file_path):
              """Load YAML file that may contain multiple documents (--- separators)."""
              with open(file_path, 'r', encoding='utf-8') as f:
                  content = f.read()
              
              documents = []
              try:
                  for doc in yaml.safe_load_all(content):
                      if doc is not None:
                          documents.append(doc)
              except yaml.YAMLError as e:
                  print(f"  ‚ö†Ô∏è YAML parse error, attempting recovery: {e}")
                  # Try single document load
                  try:
                      doc = yaml.safe_load(content)
                      if doc:
                          documents.append(doc)
                  except (yaml.YAMLError, ValueError):
                      pass
              
              return documents

          def load_file(file_path):
              """Load JSON or YAML file, returning list of documents."""
              if file_path.suffix.lower() == '.json':
                  with open(file_path, 'r', encoding='utf-8') as f:
                      data = json.load(f)
                  return [data] if not isinstance(data, list) else data
              else:
                  return load_yaml_multi_doc(file_path)

          def save_json(file_path, data):
              """Save data as pretty-printed JSON."""
              file_path.parent.mkdir(parents=True, exist_ok=True)
              with open(file_path, 'w', encoding='utf-8') as f:
                  json.dump(data, f, indent=2, ensure_ascii=False)
                  f.write('\n')

          def detect_schema_from_id_prefix(id_value):
              """Detect schema type from ID prefix patterns."""
              if not id_value:
                  return None
              id_str = str(id_value).upper()
              
              # Evidence item patterns: EV-, EVID-, evidence-
              if id_str.startswith(('EV-', 'EVID-', 'EVID_')) or 'EVIDENCE' in id_str:
                  return "evidence_item"
              
              # Money flow patterns: MF-, FLOW-, flow_id
              if id_str.startswith(('MF-', 'FLOW-', 'FLOW_')) or 'MONEY' in id_str:
                  return "money_flow"
              
              # Authority reference patterns: AR-, AUTH-, authority-
              if id_str.startswith(('AR-', 'AUTH-', 'AUTHORITY-')):
                  return "authority_reference"
              
              # Field validation patterns: FV-, FIELD-
              if id_str.startswith(('FV-', 'FIELD-')):
                  return "field_validation"
              
              return None

          def detect_schema_type(data):
              """Detect the schema type from content."""
              if not isinstance(data, dict):
                  return None
              
              # Check for wrapper keys first (money_flow: { ... })
              for wrapper_key in SCHEMA_CONFIG:
                  if wrapper_key in data and isinstance(data[wrapper_key], dict):
                      return wrapper_key
              
              # Check for ID fields
              id_to_type = {
                  "evidence_id": "evidence_item",
                  "flow_id": "money_flow",
                  "fv_id": "field_validation",
                  "authority_id": "authority_reference"
              }
              for id_field, schema_type in id_to_type.items():
                  if id_field in data:
                      return schema_type
              
              # Check explicit schema field
              if 'schema' in data and data['schema'] in SCHEMA_CONFIG:
                  return data['schema']
              
              # Try to detect from any ID-like field value
              for key, value in data.items():
                  if isinstance(value, str) and ('id' in key.lower() or key.lower().endswith('_id')):
                      detected = detect_schema_from_id_prefix(value)
                      if detected:
                          return detected
              
              return None

          def detect_schema_from_path(file_path):
              """Detect schema type from file path or filename."""
              path_str = str(file_path).lower()
              filename = Path(file_path).stem.upper()
              
              # Check if path contains schema type name
              for schema_type in SCHEMA_CONFIG:
                  if schema_type in path_str:
                      return schema_type
              
              # Try to detect from filename prefix
              detected = detect_schema_from_id_prefix(filename)
              if detected:
                  return detected
              
              return None

          def fix_authority_reference(data, original_id=None):
              """Fix authority_reference data."""
              fixed = {}
              fixed["authority_id"] = data.get("authority_id") or original_id or f"AR-GEN-{datetime.now().strftime('%Y%m%d%H%M%S')}"
              
              if "authority_type" in data:
                  at = data["authority_type"]
                  fixed["authority_type"] = at if at in ["statute", "regulation", "policy"] else "statute"
              else:
                  fixed["authority_type"] = "statute"
              
              fixed["citation"] = data.get("citation") or data.get("statute") or f"See {fixed['authority_id']}"
              
              ab = data.get("administering_body", "Unknown")
              fixed["administering_body"] = ", ".join(str(item) for item in ab) if isinstance(ab, list) else str(ab)
              
              gov = data.get("governs", ["General governance"])
              fixed["governs"] = [str(g) for g in gov] if isinstance(gov, list) else [str(gov)]
              
              if "effects" in data and isinstance(data["effects"], dict):
                  fixed["effects"] = {
                      "access_limiting": bool(data["effects"].get("access_limiting", False)),
                      "appeal_mechanism": bool(data["effects"].get("appeal_mechanism", False))
                  }
              else:
                  fixed["effects"] = {"access_limiting": False, "appeal_mechanism": False}
              
              es = data.get("editor_status", "pending")
              fixed["editor_status"] = es if es in ["pending", "accepted", "corrected", "nullified"] else "pending"
              
              return fixed

          def fix_evidence_item(data, original_id=None):
              """Fix evidence_item data."""
              fixed = {}
              fixed["evidence_id"] = data.get("evidence_id") or original_id or f"EV-GEN-{datetime.now().strftime('%Y%m%d%H%M%S')}"
              
              if "section" in data:
                  fixed["section"] = data["section"]
              else:
                  match = re.search(r'EV-([A-Z]+)-', str(fixed["evidence_id"]))
                  fixed["section"] = match.group(1).lower() if match else "general"
              
              fixed["claim_summary"] = data.get("claim_summary") or data.get("statement") or data.get("description", "Evidence item pending detailed summary")[:500]
              
              et = data.get("evidence_type", "field_validation")
              fixed["evidence_type"] = et if et in ["statute", "budget", "grant", "administrative_rule", "field_validation"] else "field_validation"
              
              if "source" in data and isinstance(data["source"], dict):
                  fixed["source"] = {
                      "title": data["source"].get("title", "Source document"),
                      "issuing_body": data["source"].get("issuing_body", "Government entity")
                  }
                  if "url" in data["source"]:
                      fixed["source"]["url"] = data["source"]["url"]
              else:
                  fixed["source"] = {"title": "Source document", "issuing_body": "Government entity"}
              
              cl = data.get("confidence_level", "medium")
              fixed["confidence_level"] = cl if cl in ["high", "medium", "low"] else "medium"
              
              es = data.get("editor_status", "pending")
              fixed["editor_status"] = es if es in ["pending", "accepted", "corrected", "nullified"] else "pending"
              
              return fixed

          def fix_money_flow(data, original_id=None):
              """Fix money_flow data."""
              fixed = {}
              fixed["flow_id"] = data.get("flow_id") or original_id or f"MF-GEN-{datetime.now().strftime('%Y%m%d%H%M%S')}"
              fixed["source"] = str(data.get("source", "Unknown source"))
              fixed["intermediary"] = str(data.get("intermediary") or "Direct")
              fixed["destination"] = str(data.get("destination", "Unknown destination"))
              
              amt = data.get("amount", 0)
              if isinstance(amt, str):
                  amt = re.sub(r'[^\d.]', '', amt)
                  try:
                      fixed["amount"] = float(amt) if '.' in str(amt) else int(amt or 0)
                  except:
                      fixed["amount"] = 0
              else:
                  fixed["amount"] = float(amt) if amt else 0
              
              ft = data.get("fund_type", "state")
              fixed["fund_type"] = ft if ft in ["state", "federal", "settlement"] else "state"
              
              fy = str(data.get("fiscal_year", "2025-2026"))
              if not re.match(r'^[0-9]{4}(-[0-9]{4})?$', fy):
                  match = re.search(r'(\d{4})', fy)
                  fixed["fiscal_year"] = match.group(1) if match else "2025-2026"
              else:
                  fixed["fiscal_year"] = fy
              
              if "restrictions" in data and isinstance(data["restrictions"], dict):
                  med = data["restrictions"].get("medicaid")
                  dhs = data["restrictions"].get("dhs_controlled")
                  fixed["restrictions"] = {
                      "medicaid": med if isinstance(med, bool) else False,
                      "dhs_controlled": dhs if isinstance(dhs, bool) else False
                  }
              else:
                  fixed["restrictions"] = {"medicaid": False, "dhs_controlled": False}
              
              fixed["statutory_basis"] = str(data.get("statutory_basis", "Not specified"))
              
              es = data.get("editor_status", "pending")
              fixed["editor_status"] = es if es in ["pending", "accepted", "corrected", "nullified"] else "pending"
              
              return fixed

          def fix_field_validation(data, original_id=None):
              """Fix field_validation data."""
              fixed = {}
              fixed["fv_id"] = data.get("fv_id") or original_id or f"FV-GEN-{datetime.now().strftime('%Y%m%d%H%M%S')}"
              fixed["jurisdiction"] = str(data.get("jurisdiction", "Arkansas"))
              fixed["validating_entity"] = str(data.get("validating_entity", "Clearlane"))
              
              if "corroborator" in data:
                  fixed["corroborator"] = str(data["corroborator"])
              
              as_val = data.get("alignment_status", "open")
              fixed["alignment_status"] = as_val if as_val in ["open", "mixed", "captured"] else "open"
              
              eb = data.get("evidence_basis", ["Pending evidence collection"])
              fixed["evidence_basis"] = [str(e) for e in eb] if isinstance(eb, list) else [str(eb)]
              
              fixed["disclosure_level"] = "restricted"
              
              es = data.get("editor_status", "pending")
              fixed["editor_status"] = es if es in ["pending", "accepted", "corrected", "nullified"] else "pending"
              
              return fixed

          def fix_item(data, schema_type, original_id=None):
              """Fix an item based on its schema type."""
              # If data is wrapped (e.g., {money_flow: {...}}), unwrap it
              if schema_type in data and isinstance(data[schema_type], dict):
                  data = data[schema_type]
              
              if schema_type == "authority_reference":
                  return fix_authority_reference(data, original_id)
              elif schema_type == "evidence_item":
                  return fix_evidence_item(data, original_id)
              elif schema_type == "money_flow":
                  return fix_money_flow(data, original_id)
              elif schema_type == "field_validation":
                  return fix_field_validation(data, original_id)
              return None

          def sanitize_filename(name):
              """Sanitize a string for use as a filename."""
              if name is None:
                  name = "unknown"
              name = str(name)
              for char in ['/', '\\', ':', '*', '?', '"', '<', '>', '|']:
                  name = name.replace(char, '_')
              return name[:200] if len(name) > 200 else name

          def find_processable_sources():
              """Find all source files in invalidated directory."""
              sources = []
              
              if not INVALIDATED_DIR.exists():
                  return sources
              
              for schema_type in SCHEMA_CONFIG.keys():
                  subdir = INVALIDATED_DIR / schema_type
                  if not subdir.exists():
                      continue
                  
                  for item in subdir.iterdir():
                      if item.is_file():
                          if item.name != ".gitkeep" and item.suffix in ['.yaml', '.yml', '.json']:
                              sources.append(item)
                      elif item.is_dir():
                          # GitLab creates dirs like "MF-AR-BATCH-130-1.yaml/" containing source.yaml
                          source_yaml = item / "source.yaml"
                          if source_yaml.exists():
                              sources.append(source_yaml)
                          # Also check for 0.json
                          zero_json = item / "0.json"
                          if zero_json.exists():
                              sources.append(zero_json)
              
              return sources

          def process_source(file_path, run_log):
              """Process a single source file."""
              print(f"\nüìÑ Processing: {file_path}")
              
              # Determine if this is inside a GitLab-created directory
              parent_is_gitlab_dir = file_path.parent.suffix in ['.yaml', '.yml', '.json']
              
              # Determine schema type from path
              path_schema = detect_schema_from_path(file_path)
              
              # Extract base ID
              if parent_is_gitlab_dir:
                  base_id = file_path.parent.stem
              else:
                  base_id = file_path.stem
              
              try:
                  documents = load_file(file_path)
                  
                  if not documents:
                      print(f"  ‚ö†Ô∏è No valid documents found")
                      run_log.errors.append({"file": str(file_path), "error": "No valid documents"})
                      run_log.failed_sources.append(str(file_path))
                      return False
                  
                  output_files = []
                  output_details = []  # Track per-output file details including schema type
                  item_counter = 0
                  
                  for doc in documents:
                      if not isinstance(doc, dict):
                          continue
                      
                      schema_type = detect_schema_type(doc) or path_schema
                      
                      if not schema_type:
                          print(f"  ‚ö†Ô∏è Could not determine schema type for document")
                          continue
                      
                      # Get ID field
                      id_field = SCHEMA_CONFIG[schema_type]["id_field"]
                      
                      # Handle wrapped data
                      actual_data = doc.get(schema_type, doc) if schema_type in doc else doc
                      
                      # Get or generate ID
                      item_id = actual_data.get(id_field) or f"{base_id}-{item_counter}"
                      item_counter += 1
                      
                      # Fix the item
                      fixed_item = fix_item(doc, schema_type, item_id)
                      
                      if fixed_item:
                          target_dir = PROPOSALS_DIR / schema_type
                          safe_id = sanitize_filename(fixed_item.get(id_field, item_id))
                          output_file = target_dir / f"{safe_id}.json"
                          
                          save_json(output_file, fixed_item)
                          output_files.append(str(output_file))
                          output_details.append({
                              "file": str(output_file),
                              "schema_type": schema_type,
                              "id": fixed_item.get(id_field, item_id)
                          })
                          run_log.created_files.append(str(output_file))
                          print(f"  ‚úÖ Created: {output_file}")
                  
                  if output_files:
                      # Collect unique schema types processed
                      schema_types = list(set(d["schema_type"] for d in output_details))
                      run_log.processed_files.append({
                          "source": str(file_path),
                          "schema_types": schema_types,
                          "items_fixed": len(output_files),
                          "output_files": output_details
                      })
                      return True
                  
                  # No output files produced - mark as failed
                  run_log.failed_sources.append(str(file_path))
                  return False
                  
              except Exception as e:
                  print(f"  ‚ùå Error: {e}")
                  run_log.errors.append({"file": str(file_path), "error": str(e)})
                  run_log.failed_sources.append(str(file_path))
                  import traceback
                  traceback.print_exc()
                  return False

          def cleanup_invalidated(run_log):
              """Clean up invalidated directory - delete successfully processed sources, quarantine failed ones."""
              print("\nüßπ Cleaning up invalidated/ directory...")
              
              if not INVALIDATED_DIR.exists():
                  return
              
              # Create quarantine directory for failed sources
              quarantine_dir = ACTIONS_LOG_DIR / f"quarantine_{run_log.timestamp_str}"
              
              # Get set of successfully processed source paths
              processed_sources = set()
              for pf in run_log.processed_files:
                  src = pf.get("source", "")
                  processed_sources.add(src)
                  # Also add parent directory if source is inside a GitLab batch dir
                  src_path = Path(src)
                  if src_path.parent.suffix in ['.yaml', '.yml', '.json']:
                      processed_sources.add(str(src_path.parent))
              
              # Get set of failed source paths
              failed_sources = set(run_log.failed_sources)
              # Also add parent directories for failed sources
              for fs in list(failed_sources):
                  fs_path = Path(fs)
                  if fs_path.parent.suffix in ['.yaml', '.yml', '.json']:
                      failed_sources.add(str(fs_path.parent))
              
              for schema_type in SCHEMA_CONFIG.keys():
                  subdir = INVALIDATED_DIR / schema_type
                  if not subdir.exists():
                      continue
                  
                  for item in list(subdir.iterdir()):
                      if item.name == ".gitkeep":
                          continue
                      
                      item_str = str(item)
                      is_processed = item_str in processed_sources or any(item_str in ps for ps in processed_sources)
                      is_failed = item_str in failed_sources or any(item_str in fs for fs in failed_sources)
                      
                      try:
                          if is_processed and not is_failed:
                              # Successfully processed - safe to delete
                              if item.is_file():
                                  item.unlink()
                                  run_log.deleted_files.append(str(item))
                                  print(f"  üóëÔ∏è Deleted file: {item}")
                              elif item.is_dir():
                                  shutil.rmtree(item)
                                  run_log.deleted_dirs.append(str(item))
                                  print(f"  üóëÔ∏è Deleted directory: {item}")
                          elif is_failed:
                              # Failed to process - move to quarantine
                              quarantine_dir.mkdir(parents=True, exist_ok=True)
                              quarantine_subdir = quarantine_dir / schema_type
                              quarantine_subdir.mkdir(parents=True, exist_ok=True)
                              dest = quarantine_subdir / item.name
                              
                              if item.is_file():
                                  shutil.copy2(item, dest)
                                  item.unlink()
                              elif item.is_dir():
                                  shutil.copytree(item, dest)
                                  shutil.rmtree(item)
                              
                              run_log.quarantined_files.append({
                                  "source": str(item),
                                  "quarantine_path": str(dest)
                              })
                              print(f"  üì¶ Quarantined: {item} -> {dest}")
                          else:
                              # Not in either list (wasn't processed at all) - quarantine for safety
                              quarantine_dir.mkdir(parents=True, exist_ok=True)
                              quarantine_subdir = quarantine_dir / schema_type
                              quarantine_subdir.mkdir(parents=True, exist_ok=True)
                              dest = quarantine_subdir / item.name
                              
                              if item.is_file():
                                  shutil.copy2(item, dest)
                                  item.unlink()
                              elif item.is_dir():
                                  shutil.copytree(item, dest)
                                  shutil.rmtree(item)
                              
                              run_log.quarantined_files.append({
                                  "source": str(item),
                                  "quarantine_path": str(dest),
                                  "reason": "not_processed"
                              })
                              print(f"  üì¶ Quarantined (not processed): {item} -> {dest}")
                      except Exception as e:
                          print(f"  ‚ö†Ô∏è Could not handle {item}: {e}")
                          run_log.errors.append({"file": str(item), "error": f"Cleanup failed: {e}"})

          def reconcile_with_manifests(run_log):
              """Reconcile invalidated files against artifacts/validated, other JSON artifacts under artifacts/, and processed/ directories."""
              print("\nüîç Reconciling with artifacts and processed directories...")
              
              reconciliation = {
                  "already_in_artifacts_validated": [],
                  "already_in_artifacts_other": [],
                  "already_in_processed": [],
                  "unique_to_invalidated": [],
                  "duplicates_flagged": []
              }
              
              # Get all IDs from artifacts/validated/
              validated_ids = {}
              validated_dir = ARTIFACTS_DIR / "validated"
              if validated_dir.exists():
                  for f in validated_dir.rglob("*.json"):
                      try:
                          with open(f, 'r') as fh:
                              data = json.load(fh)
                          for id_field in ["evidence_id", "flow_id", "fv_id", "authority_id"]:
                              if id_field in data:
                                  validated_ids[data[id_field]] = str(f)
                                  break
                      except (json.JSONDecodeError, IOError, KeyError):
                          pass
              
              # Get all IDs from other artifacts (including manifest if any)
              other_artifact_ids = {}
              if ARTIFACTS_DIR.exists():
                  for f in ARTIFACTS_DIR.rglob("*.json"):
                      # Skip validated directory (already processed)
                      if "validated" in str(f):
                          continue
                      try:
                          with open(f, 'r') as fh:
                              data = json.load(fh)
                          for id_field in ["evidence_id", "flow_id", "fv_id", "authority_id"]:
                              if id_field in data:
                                  other_artifact_ids[data[id_field]] = str(f)
                                  break
                      except (json.JSONDecodeError, IOError, KeyError):
                          pass
              
              # Get all file stems from processed/
              processed_files = {}
              if PROCESSED_DIR.exists():
                  for f in PROCESSED_DIR.rglob("*"):
                      if f.is_file() and f.suffix in ['.yaml', '.yml', '.json']:
                          processed_files[f.stem] = str(f)
              
              # Check invalidated sources
              sources = find_processable_sources()
              for src in sources:
                  parent_is_gitlab_dir = src.parent.suffix in ['.yaml', '.yml', '.json']
                  base_id = src.parent.stem if parent_is_gitlab_dir else src.stem
                  
                  found_in = []
                  
                  # Check if in validated artifacts
                  if base_id in validated_ids:
                      reconciliation["already_in_artifacts_validated"].append({
                          "id": base_id,
                          "location": validated_ids[base_id]
                      })
                      found_in.append("artifacts/validated")
                  
                  # Check if in other artifacts
                  if base_id in other_artifact_ids:
                      reconciliation["already_in_artifacts_other"].append({
                          "id": base_id,
                          "location": other_artifact_ids[base_id]
                      })
                      found_in.append("artifacts")
                  
                  # Check if in processed
                  if base_id in processed_files:
                      reconciliation["already_in_processed"].append({
                          "id": base_id,
                          "location": processed_files[base_id]
                      })
                      found_in.append("processed")
                  
                  # Categorize based on where found
                  if len(found_in) > 1:
                      # True duplicate: exists in multiple places
                      reconciliation["duplicates_flagged"].append({
                          "id": base_id,
                          "source": str(src),
                          "found_in": found_in,
                          "action": "flagged_as_duplicate"
                      })
                  elif len(found_in) == 1:
                      # Exists in exactly one location - still process but flag in report
                      reconciliation["duplicates_flagged"].append({
                          "id": base_id,
                          "source": str(src),
                          "found_in": found_in,
                          "action": "already_exists_flagged_for_review"
                      })
                  else:
                      # Unique: not found anywhere, will fix and move to proposals
                      reconciliation["unique_to_invalidated"].append({
                          "id": base_id,
                          "source": str(src),
                          "action": "will_fix_and_move_to_proposals"
                      })
              
              run_log.reconciliation = reconciliation
              
              print(f"  üìä Already in artifacts/validated: {len(reconciliation['already_in_artifacts_validated'])}")
              print(f"  üìä Already in other artifacts: {len(reconciliation['already_in_artifacts_other'])}")
              print(f"  üìä Already in processed: {len(reconciliation['already_in_processed'])}")
              print(f"  üìä Duplicates flagged: {len(reconciliation['duplicates_flagged'])}")
              print(f"  üìä Unique to invalidated (will process): {len(reconciliation['unique_to_invalidated'])}")

          def main():
              print("=" * 70)
              print("üîß Fix Invalidated Files - GitHub Actions Workflow")
              print(f"‚è∞ Started: {datetime.now(timezone.utc).isoformat()}")
              print("=" * 70)
              
              # Initialize run log
              run_log = RunLog()
              
              # Capture before state
              print("\nüì∏ Capturing before-state inventory...")
              run_log.capture_before_state()
              print(f"  Found {run_log.before_state['total_files']} files in {run_log.before_state['total_dirs']} directories")
              
              # Reconcile first
              reconcile_with_manifests(run_log)
              
              # Find sources
              sources = find_processable_sources()
              print(f"\nüìÇ Found {len(sources)} source files to process")
              
              if not sources:
                  print("\n‚ú® No files to process - invalidated/ is already clean")
                  run_log.save()
                  
                  # Set output for GitHub Actions
                  with open(os.environ.get('GITHUB_OUTPUT', '/dev/null'), 'a') as f:
                      f.write("has_changes=false\n")
                  return
              
              # Process each source
              processed_count = 0
              for source in sources:
                  if process_source(source, run_log):
                      processed_count += 1
              
              # Cleanup invalidated directory completely
              cleanup_invalidated(run_log)
              
              # Save run log
              log_file = run_log.save()
              
              # Print summary
              print("\n" + "=" * 70)
              print("üìä PROCESSING SUMMARY")
              print("=" * 70)
              print(f"  Files processed:     {processed_count}")
              print(f"  Files created:       {len(run_log.created_files)}")
              print(f"  Files deleted:       {len(run_log.deleted_files)}")
              print(f"  Directories deleted: {len(run_log.deleted_dirs)}")
              print(f"  Files quarantined:   {len(run_log.quarantined_files)}")
              print(f"  Failed sources:      {len(run_log.failed_sources)}")
              print(f"  Errors:              {len(run_log.errors)}")
              print(f"  Run log:             {log_file}")
              print("=" * 70)
              
              # Set outputs for GitHub Actions
              has_changes = len(run_log.created_files) > 0 or len(run_log.deleted_files) > 0 or len(run_log.deleted_dirs) > 0
              proposals_changed = any('proposals/' in f for f in run_log.created_files)
              
              with open(os.environ.get('GITHUB_OUTPUT', '/dev/null'), 'a') as f:
                  f.write(f"has_changes={'true' if has_changes else 'false'}\n")
                  f.write(f"proposals_changed={'true' if proposals_changed else 'false'}\n")
                  f.write(f"files_created={len(run_log.created_files)}\n")
                  f.write(f"files_deleted={len(run_log.deleted_files)}\n")

          if __name__ == "__main__":
              main()
          PYTHON_SCRIPT
      
      - name: Commit and push changes
        run: |
          set -e
          git config --local user.name "github-actions[bot]"
          git config --local user.email "github-actions[bot]@users.noreply.github.com"
          
          # Stage all changes
          git add proposals/ reports/ invalidated/
          
          if git diff --staged --quiet; then
            echo "No changes to commit"
            exit 0
          fi
          
          # Check what changed
          proposals_changed=$(git diff --staged --name-only -- proposals/ | wc -l)
          
          if [ "$proposals_changed" -gt 0 ]; then
            # Files moved to proposals - trigger GitLab pipeline (NO [skip ci])
            git commit -m "fix: process invalidated schemas and move to proposals for validation"
            echo "‚úÖ Committing with GitLab trigger (proposals changed)"
          else
            # Only internal changes (reports, cleanup) - skip CI
            git commit -m "chore: cleanup invalidated directory and update run logs [skip ci]"
            echo "‚úÖ Committing with [skip ci] (internal changes only)"
          fi
          
          # Push with retry
          for i in 1 2 3; do
            if git push; then
              echo "‚úÖ Changes pushed successfully"
              exit 0
            else
              echo "‚ö†Ô∏è Push attempt $i failed, retrying..."
              if ! git pull --rebase origin main; then
                echo "‚ö†Ô∏è Pull from main failed, trying current branch..."
                if ! git pull --rebase origin $(git rev-parse --abbrev-ref HEAD); then
                  echo "‚ö†Ô∏è Pull from current branch also failed, continuing..."
                fi
              fi
              sleep 2
            fi
          done
          
          echo "‚ùå Failed to push after 3 attempts"
          exit 1
